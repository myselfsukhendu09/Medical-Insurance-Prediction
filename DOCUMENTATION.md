# Project Documentation

## ðŸ“š Complete Documentation for Medical Insurance Cost Prediction

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [Module Documentation](#module-documentation)
4. [API Reference](#api-reference)
5. [Data Pipeline](#data-pipeline)
6. [Model Details](#model-details)
7. [Web Application](#web-application)
8. [Deployment](#deployment)
9. [Best Practices](#best-practices)

---

## Project Overview

### Purpose
This project predicts medical insurance costs based on patient demographics and health indicators using machine learning regression models.

### Key Features
- **13 Regression Models**: Comprehensive model comparison
- **Automated Pipeline**: End-to-end ML workflow
- **Interactive Web UI**: User-friendly prediction interface
- **Extensive EDA**: Detailed exploratory data analysis
- **Hyperparameter Tuning**: Optimized model performance
- **Production Ready**: Clean, modular, documented code

### Technology Stack
- **Backend**: Python 3.8+, Flask
- **ML Libraries**: scikit-learn, XGBoost, LightGBM, CatBoost
- **Data Processing**: Pandas, NumPy
- **Visualization**: Matplotlib, Seaborn, Plotly
- **Frontend**: HTML5, CSS3, JavaScript (Vanilla)

---

## Architecture

### Project Structure
```
medical-insurance-prediction/
â”‚
â”œâ”€â”€ data/                      # Data storage
â”‚   â”œâ”€â”€ raw/                   # Original data
â”‚   â””â”€â”€ processed/             # Processed data
â”‚
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py         # Data loading
â”‚   â”œâ”€â”€ eda.py                 # Exploratory analysis
â”‚   â”œâ”€â”€ preprocessing.py       # Data cleaning
â”‚   â”œâ”€â”€ feature_engineering.py # Feature creation
â”‚   â”œâ”€â”€ model_training.py      # Model training
â”‚   â”œâ”€â”€ model_evaluation.py    # Model evaluation
â”‚   â”œâ”€â”€ utils.py               # Utility functions
â”‚   â””â”€â”€ main.py                # Main pipeline
â”‚
â”œâ”€â”€ models/                    # Saved models
â”œâ”€â”€ results/                   # Results and figures
â”œâ”€â”€ web_app/                   # Web application
â”‚   â”œâ”€â”€ app.py                 # Flask application
â”‚   â”œâ”€â”€ templates/             # HTML templates
â”‚   â””â”€â”€ static/                # CSS, JS, images
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ setup.py                   # Package setup
â”œâ”€â”€ README.md                  # Main documentation
â”œâ”€â”€ QUICKSTART.md              # Quick start guide
â””â”€â”€ DOCUMENTATION.md           # This file
```

### Data Flow
```
Raw Data â†’ Data Loader â†’ Preprocessing â†’ Feature Engineering â†’ 
Model Training â†’ Model Evaluation â†’ Saved Models â†’ Web App
```

---

## Module Documentation

### 1. data_loader.py

**Purpose**: Load and perform initial exploration of data

**Key Classes**:
- `DataLoader`: Main class for data loading

**Key Methods**:
- `load_data(file_path)`: Load CSV data
- `get_basic_info()`: Display dataset information
- `get_column_info(column_name)`: Detailed column analysis
- `save_processed_data(output_path)`: Save data to CSV

**Usage**:
```python
from data_loader import DataLoader

loader = DataLoader('data/raw/insurance.csv')
df = loader.load_data()
loader.get_basic_info()
```

### 2. eda.py

**Purpose**: Comprehensive exploratory data analysis

**Key Classes**:
- `EDAAnalyzer`: Main class for EDA

**Key Methods**:
- `plot_target_distribution()`: Visualize target variable
- `plot_numerical_distributions()`: Plot numerical features
- `plot_categorical_distributions()`: Plot categorical features
- `plot_correlation_heatmap()`: Correlation analysis
- `plot_feature_vs_target()`: Feature relationships
- `analyze_outliers()`: Outlier detection
- `generate_eda_report()`: Complete EDA report

**Usage**:
```python
from eda import EDAAnalyzer

eda = EDAAnalyzer(df)
eda.generate_eda_report()
```

### 3. preprocessing.py

**Purpose**: Data cleaning and preprocessing

**Key Classes**:
- `DataPreprocessor`: Main preprocessing class

**Key Methods**:
- `check_missing_values()`: Identify missing data
- `handle_missing_values(strategy)`: Fill missing values
- `detect_outliers_iqr()`: IQR-based outlier detection
- `detect_outliers_zscore()`: Z-score outlier detection
- `handle_outliers(method)`: Treat outliers
- `check_duplicates()`: Find duplicate rows
- `remove_duplicates()`: Remove duplicates

**Usage**:
```python
from preprocessing import DataPreprocessor

preprocessor = DataPreprocessor(df)
preprocessor.handle_missing_values(strategy='mean')
preprocessor.handle_outliers(method='cap')
```

### 4. feature_engineering.py

**Purpose**: Feature transformation and creation

**Key Classes**:
- `FeatureEngineer`: Main feature engineering class

**Key Methods**:
- `encode_categorical_label()`: Label encoding
- `encode_categorical_onehot()`: One-hot encoding
- `check_skewness()`: Analyze feature skewness
- `handle_skewness(method)`: Transform skewed features
- `scale_features(method)`: Feature scaling
- `create_interaction_features()`: Create interactions
- `save_encoders_scalers()`: Save preprocessing objects

**Usage**:
```python
from feature_engineering import FeatureEngineer

fe = FeatureEngineer(df)
fe.encode_categorical_label(['sex', 'smoker', 'region'])
fe.scale_features(method='standard')
fe.create_interaction_features()
```

### 5. model_training.py

**Purpose**: Train and tune ML models

**Key Classes**:
- `ModelTrainer`: Main training class

**Key Methods**:
- `initialize_models()`: Set up all models
- `train_all_models()`: Train all models
- `cross_validate_models()`: K-fold cross-validation
- `tune_random_forest()`: RF hyperparameter tuning
- `tune_gradient_boosting()`: GB hyperparameter tuning
- `tune_xgboost()`: XGBoost hyperparameter tuning
- `save_models()`: Save trained models

**Models Included**:
1. Linear Regression
2. Ridge Regression
3. Lasso Regression
4. ElasticNet
5. Decision Tree
6. Random Forest
7. Gradient Boosting
8. AdaBoost
9. SVR
10. KNN
11. XGBoost
12. LightGBM
13. CatBoost

**Usage**:
```python
from model_training import ModelTrainer

trainer = ModelTrainer(X_train, X_test, y_train, y_test)
trainer.train_all_models()
trainer.cross_validate_models(cv=5)
trainer.save_models()
```

### 6. model_evaluation.py

**Purpose**: Evaluate and compare models

**Key Classes**:
- `ModelEvaluator`: Main evaluation class

**Key Methods**:
- `evaluate_model(model, name)`: Evaluate single model
- `evaluate_all_models()`: Evaluate all models
- `print_detailed_results()`: Detailed metrics
- `print_comparison_table()`: Comparison table
- `plot_model_comparison()`: Comparison visualizations
- `plot_predictions_vs_actual()`: Prediction plots
- `plot_residuals()`: Residual analysis
- `save_results()`: Save evaluation results

**Metrics Calculated**:
- MAE (Mean Absolute Error)
- MSE (Mean Squared Error)
- RMSE (Root Mean Squared Error)
- RÂ² Score
- Adjusted RÂ² Score
- MAPE (Mean Absolute Percentage Error)
- Explained Variance Score
- Overfitting Detection

**Usage**:
```python
from model_evaluation import ModelEvaluator

evaluator = ModelEvaluator(models, X_train, X_test, y_train, y_test)
evaluator.evaluate_all_models()
evaluator.print_comparison_table()
evaluator.plot_model_comparison()
```

---

## API Reference

### Web Application Endpoints

#### GET /
- **Description**: Render home page
- **Returns**: HTML page

#### POST /predict
- **Description**: Make insurance cost prediction
- **Request Body**:
  ```json
  {
    "age": 35,
    "sex": "male",
    "bmi": 27.5,
    "children": 2,
    "smoker": "no",
    "region": "northeast"
  }
  ```
- **Response**:
  ```json
  {
    "prediction": 5234.56,
    "formatted_prediction": "$5,234.56",
    "inputs": {...},
    "insights": [...]
  }
  ```
- **Error Response**:
  ```json
  {
    "error": "Error message"
  }
  ```

#### GET /health
- **Description**: Health check endpoint
- **Returns**:
  ```json
  {
    "status": "healthy",
    "model_loaded": true
  }
  ```

---

## Data Pipeline

### Step-by-Step Process

1. **Data Loading**
   - Load CSV file
   - Initial exploration
   - Data type detection

2. **Exploratory Data Analysis**
   - Distribution analysis
   - Correlation analysis
   - Outlier detection
   - Visualization generation

3. **Preprocessing**
   - Missing value handling
   - Duplicate removal
   - Outlier treatment

4. **Feature Engineering**
   - Categorical encoding
   - Feature scaling
   - Skewness handling
   - Interaction features

5. **Model Training**
   - Train/test split
   - Model initialization
   - Training all models
   - Cross-validation

6. **Model Evaluation**
   - Metric calculation
   - Model comparison
   - Visualization
   - Best model selection

7. **Deployment**
   - Model saving
   - Web app integration
   - API creation

---

## Model Details

### Model Selection Criteria

Models were chosen based on:
- **Variety**: Linear, tree-based, ensemble, distance-based
- **Performance**: Known effectiveness for regression
- **Interpretability**: Balance between accuracy and explainability
- **Scalability**: Suitable for production deployment

### Expected Performance

Based on typical insurance datasets:

| Model | Expected RÂ² | Expected RMSE | Training Time |
|-------|-------------|---------------|---------------|
| Linear Regression | 0.70-0.75 | $6,000-$7,000 | Fast |
| Random Forest | 0.83-0.87 | $4,500-$5,500 | Medium |
| Gradient Boosting | 0.85-0.88 | $4,200-$5,000 | Medium |
| XGBoost | 0.86-0.90 | $4,000-$4,800 | Medium |
| LightGBM | 0.85-0.89 | $4,100-$4,900 | Fast |

### Feature Importance

Typical feature importance ranking:
1. **Smoker** (40-50% importance)
2. **Age** (20-25% importance)
3. **BMI** (15-20% importance)
4. **BMI Ã— Smoker** (10-15% importance)
5. **Children** (3-5% importance)
6. **Region** (1-3% importance)

---

## Web Application

### Frontend Features

- **Modern UI**: Dark theme with gradients
- **Responsive**: Works on all devices
- **Animations**: Smooth transitions and effects
- **Validation**: Client-side input validation
- **Insights**: Contextual information display

### Backend Features

- **Flask Framework**: Lightweight and efficient
- **RESTful API**: Clean endpoint design
- **Error Handling**: Comprehensive error messages
- **Model Loading**: Automatic best model selection
- **Preprocessing**: Input transformation pipeline

### Security Considerations

- Input validation
- Error handling
- No data storage
- CORS configuration
- Rate limiting (recommended for production)

---

## Deployment

### Local Deployment

```bash
cd web_app
python app.py
```

### Production Deployment Options

1. **Heroku**
   ```bash
   heroku create medical-insurance-predictor
   git push heroku main
   ```

2. **AWS Elastic Beanstalk**
   ```bash
   eb init
   eb create
   eb deploy
   ```

3. **Google Cloud Run**
   ```bash
   gcloud run deploy
   ```

4. **Docker**
   ```dockerfile
   FROM python:3.9
   COPY . /app
   WORKDIR /app
   RUN pip install -r requirements.txt
   CMD ["python", "web_app/app.py"]
   ```

---

## Best Practices

### Code Quality

- **PEP 8**: Follow Python style guide
- **Docstrings**: Document all functions
- **Type Hints**: Use type annotations
- **Error Handling**: Comprehensive try-except blocks
- **Logging**: Use logging instead of print

### Data Science

- **Reproducibility**: Set random seeds
- **Validation**: Use cross-validation
- **Feature Engineering**: Domain knowledge
- **Model Selection**: Compare multiple models
- **Overfitting**: Monitor train/test performance

### Production

- **Version Control**: Use Git
- **Testing**: Write unit tests
- **Documentation**: Keep docs updated
- **Monitoring**: Track model performance
- **Retraining**: Regular model updates

---

## Troubleshooting

### Common Issues

1. **Import Errors**
   - Ensure virtual environment is activated
   - Check Python version (3.8+)
   - Reinstall requirements

2. **Memory Issues**
   - Reduce dataset size
   - Use memory-efficient dtypes
   - Process in batches

3. **Model Performance**
   - Check data quality
   - Try feature engineering
   - Tune hyperparameters
   - Collect more data

4. **Web App Issues**
   - Verify model is trained
   - Check file paths
   - Review Flask logs
   - Test API endpoints

---

## Future Enhancements

### Planned Features

- [ ] Deep learning models
- [ ] AutoML integration
- [ ] Real-time predictions
- [ ] Model monitoring dashboard
- [ ] A/B testing framework
- [ ] API authentication
- [ ] Database integration
- [ ] Batch prediction support
- [ ] Model explainability (SHAP)
- [ ] Mobile application

---

## Contributing

### How to Contribute

1. Fork the repository
2. Create feature branch
3. Make changes
4. Write tests
5. Update documentation
6. Submit pull request

### Code Review Process

- All PRs require review
- Tests must pass
- Documentation must be updated
- Code must follow style guide

---

## License

MIT License - See LICENSE file for details

---

## Contact & Support

- **Email**: your.email@example.com
- **GitHub**: github.com/yourusername
- **Issues**: github.com/yourusername/medical-insurance-prediction/issues

---

**Last Updated**: January 2026
**Version**: 1.0.0
